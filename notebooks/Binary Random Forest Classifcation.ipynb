{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQI Random Forest Classification Exercise for Binary Data\n",
    "\n",
    "The following uses the binary score - participation (which can only be 0 or 1), to create a random forest classification model. It splits the labelled data into a training set (80%) and a testing set (20%). In the first pass (April 21, 2017), it has a 90% success rate in predicting the participation score of the test comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('combined_scored.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "The first step to random forest classification is splitting the labelled data into two subsets - the training and the testing set. The training set will be used to build the model, and the testing set will be used to evaluate the accuracy of the predictions.\n",
    "\n",
    "The comments are then cleaned by extracting all non-alpha characters and removing stopwords. In this case, the stopwords have been imported from the python nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eleonoreft/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data = data_df[['comment', 'participation']]\n",
    "\n",
    "train, test = train_test_split(data, train_size = 0.8, random_state = 44)\n",
    "\n",
    "def comment_to_words(raw_comment):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_comment)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "clean_train_comments = []\n",
    "\n",
    "train[\"cleaned_comment\"] = train[\"comment\"].apply(lambda x: comment_to_words(x))\n",
    "\n",
    "for i in train[\"cleaned_comment\"].values:\n",
    "    clean_train_comments.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word features\n",
    "Once the stopwords are removed, the remaining words are transformed into bag of words features. In the bag of words method, each word is evaluated independently from the others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number of comments, number of words/features)\n",
      "(182, 2026)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_comments)\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print '(Number of comments, number of words/features)'\n",
    "print train_data_features.shape\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist = np.sum(train_data_features, axis = 0)\n",
    "\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print count, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the random forest classifier\n",
    "The Random Forest Classifier is then created. This classifier is based on a decision tree algorithm where the trees are aggregated to select a random subset of features. Decision trees have a tendency to overfit the data and to go very deep. Creating a random forest averages the predicitions from each decision tree. \n",
    "\n",
    "The classifier is created, in this case with 100 decision trees. It is trained on the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(train_data_features, train[\"participation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the trained random forest\n",
    "\n",
    "Once the random forest is trained, the test data is prepared and used to evaluate the classifier. The result shows a table of actual vs predicted participation scores. \n",
    "\n",
    "The confusion matrix shows a 90% success rate in this first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   1\n",
      "Actual       \n",
      "0           5\n",
      "1          41\n",
      "     actual_participation  predicted_participation\n",
      "221                     1                        1\n",
      "186                     1                        1\n",
      "35                      1                        1\n",
      "106                     1                        1\n",
      "110                     1                        1\n",
      "30                      1                        1\n",
      "137                     1                        1\n",
      "105                     1                        1\n",
      "58                      1                        1\n",
      "208                     1                        1\n",
      "203                     1                        1\n",
      "82                      1                        1\n",
      "210                     1                        1\n",
      "135                     1                        1\n",
      "178                     0                        1\n",
      "77                      1                        1\n",
      "43                      1                        1\n",
      "174                     1                        1\n",
      "125                     1                        1\n",
      "61                      1                        1\n",
      "170                     1                        1\n",
      "9                       1                        1\n",
      "207                     1                        1\n",
      "54                      1                        1\n",
      "23                      1                        1\n",
      "211                     1                        1\n",
      "162                     1                        1\n",
      "5                       1                        1\n",
      "175                     0                        1\n",
      "7                       1                        1\n",
      "79                      1                        1\n",
      "3                       1                        1\n",
      "102                     1                        1\n",
      "134                     1                        1\n",
      "46                      1                        1\n",
      "113                     1                        1\n",
      "191                     1                        1\n",
      "44                      1                        1\n",
      "188                     0                        1\n",
      "51                      1                        1\n",
      "94                      1                        1\n",
      "213                     1                        1\n",
      "73                      1                        1\n",
      "155                     0                        1\n",
      "19                      1                        1\n",
      "153                     0                        1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eleonoreft/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Run trained Random Forest on test data set\n",
    "\n",
    "clean_test_comments = []\n",
    "\n",
    "test[\"cleaned_comment\"] = test[\"comment\"].apply(lambda x: comment_to_words(x))\n",
    "\n",
    "for i in test[\"cleaned_comment\"].values:\n",
    "    clean_test_comments.append(i)\n",
    "    \n",
    "test_data_features = vectorizer.transform(clean_test_comments)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "output = pd.DataFrame(data={\"actual_participation\": test[\"participation\"], \"predicted_participation\": result})\n",
    "\n",
    "# Create confusion matrix\n",
    "\n",
    "df_confusion = pd.crosstab(output['actual_participation'], output['predicted_participation'], rownames=['Actual'], \\\n",
    "                           colnames=['Predicted'])\n",
    "\n",
    "print df_confusion\n",
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
