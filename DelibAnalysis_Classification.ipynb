{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DelibAnalysis Classification\n",
    "\n",
    "The following script implements the random forests classifier in order to predict the Discourse Quality Index (DQI) category of online comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to process data\n",
    "\n",
    "def process_labelled_data(source_csv):\n",
    "    data_df = pd.read_csv(source_csv)\n",
    "    indicators = ['participation', 'level_of_justification', 'content_of_justification', 'respect', 'counterarguments', 'constructive_politics']\n",
    "    data_df['dqi'] = data_df[indicators].sum(axis=1)\n",
    "    data_df['dqi_groups'] = data_df.dqi.map( lambda x: 0 if x <= 5 else 1 if (x > 5 and x <=10) else 2)\n",
    "    data = data_df[['dqi', 'comment', 'dqi_groups','fb_comment', 'live_th', 'blog_comment']]\n",
    "    return data\n",
    "\n",
    "def comment_to_words(raw_comment):\n",
    "    try:\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_comment)\n",
    "        words = letters_only.lower().split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        meaningful_words = [w for w in words if not w in stops]\n",
    "        return(\" \".join(meaningful_words))\n",
    "    except TypeError: \n",
    "        print raw_comment  \n",
    "\n",
    "def append_features(input_matrix,input_feature):\n",
    "    count = 0\n",
    "    new_matrix = np.zeros(shape=(input_matrix.shape[0], input_matrix.shape[1]+1))\n",
    "    for i in range(0, len(input_feature)):\n",
    "        new_matrix[i] = np.append(input_matrix[i], input_feature[i])\n",
    "    return new_matrix\n",
    "\n",
    "char_dict = {'less_than_30_chars': (30,0),'between_30_and_100_chars': (100,30), 'between_100_and_300_chars': (300,100),\n",
    "'between_300_and_800_chars': (800, 300), 'between_800_and_1500_chars': (1500, 800), \n",
    "'between_1500_and_3000_chars': (3000, 1500), 'more_than_3000_chars': (1000000, 3000)}\n",
    "\n",
    "def add_character_counts(data, chars):\n",
    "    data['char_count'] = data['comment'].apply(lambda x: len(x))\n",
    "    for k, v in chars.items():\n",
    "        data[k] = data.char_count.map(lambda x: 1 if (x <= v[0] and x > v[1]) else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and process labelled data\n",
    "\n",
    "labelled_data = process_labelled_data(\"combined_scored.csv\")\n",
    "labelled_data[\"cleaned_comment\"] = labelled_data[\"comment\"].apply(lambda x: comment_to_words(x))\n",
    "labelled_data = add_character_counts(labelled_data, char_dict)\n",
    "print(labelled_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier \n",
    "\n",
    "train, test = train_test_split(labelled_data, train_size = 0.8, random_state = 44)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, \\\n",
    "                             max_features = 6000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(train[\"cleaned_comment\"])\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print train_data_features.shape\n",
    "\n",
    "\n",
    "quantitative_features = [\"fb_comment\", \"live_th\", \"blog_comment\", \"less_than_30_chars\", \"between_30_and_100_chars\",\n",
    "                        \"between_100_and_300_chars\", \"between_300_and_800_chars\", \"between_800_and_1500_chars\",\n",
    "                        \"between_1500_and_3000_chars\", \"more_than_3000_chars\"]\n",
    "for i in quantitative_features:\n",
    "    train_data_features = append_features(train_data_features, train[i].as_matrix())\n",
    "\n",
    "print '(Number of comments, number of features)'\n",
    "print train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "forest = RandomForestClassifier(n_jobs=-1, n_estimators=24, criterion=\"entropy\", max_depth=17, warm_start=True, \n",
    "                                max_features=2000, bootstrap=True)\n",
    "y,_ = pd.factorize(train['dqi_groups'])\n",
    "forest.fit(train_data_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the top features used by the classifier by importance\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "vocab = vectorizer.get_feature_names()\n",
    "for i in quantitative_features:\n",
    "    vocab.append(i)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "feature_importance_df = pd.DataFrame(data=None, columns = ['Feature name', 'Importance'])\n",
    "for f in range(0,50):\n",
    "    feature_importance_df.loc[f+1] = [vocab[indices[f]], importances[indices[f]]]\n",
    "\n",
    "plt = feature_importance_df.plot(kind=\"barh\", figsize=(10,10), color=\"purple\")\n",
    "plt.set_yticklabels(feature_importance_df[\"Feature name\"])\n",
    "plt.invert_yaxis()\n",
    "plt.set_title(\"Top 50 features by importance\")\n",
    "plt.set_xlabel(\"Importance\")\n",
    "plt.set_ylabel(\"Feature name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data\n",
    "\n",
    "test_data_features = vectorizer.transform(test[\"cleaned_comment\"])\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "print test_data_features.shape\n",
    "\n",
    "for i in quantitative_features:\n",
    "    test_data_features = append_features(test_data_features, test[i].as_matrix())\n",
    "\n",
    "print '(Number of comments, number of features)'\n",
    "print test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier by predicting the score of the test group\n",
    "\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "output = pd.DataFrame(data={\"actual_dqi\": test[\"dqi_groups\"], \"predicted_dqi\": result})\n",
    "\n",
    "# Create confusion matrix\n",
    "\n",
    "print pd.crosstab(output['actual_dqi'], output['predicted_dqi'], rownames=['Actual'], \\\n",
    "                           colnames=['Predicted'])\n",
    "\n",
    "print '\\n*Classification Report:\\n', classification_report(output['actual_dqi'], output['predicted_dqi']) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
